{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e502def-944c-4e88-8ee6-e6264123cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0418b-6a9d-49cf-917b-3ece2d8c9914",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/twshen/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('../data/xbar/1/xbar.json.gz','rb') as f:\n",
    "    design = json.loads(f.read().decode('utf-8'))\n",
    "    \n",
    "instances = pd.DataFrame(design['instances'])\n",
    "nets = pd.DataFrame(design['nets'])\n",
    "\n",
    "conn=np.load('../data/xbar/1/xbar_connectivity.npz')\n",
    "A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "A = A.__mul__(A.T)\n",
    "\n",
    "def buildBST(array,start=0,finish=-1):\n",
    "    if finish<0:\n",
    "        finish = len(array)\n",
    "    mid = (start + finish) // 2\n",
    "    if mid-start==1:\n",
    "        ltl=start\n",
    "    else:\n",
    "        ltl=buildBST(array,start,mid)\n",
    "    \n",
    "    if finish-mid==1:\n",
    "        gtl=mid\n",
    "    else:\n",
    "        gtl=buildBST(array,mid,finish)\n",
    "        \n",
    "    return((array[mid],ltl,gtl))\n",
    "\n",
    "congestion_data = np.load('../data/xbar/1/xbar_congestion.npz')\n",
    "xbst=buildBST(congestion_data['xBoundaryList'])\n",
    "ybst=buildBST(congestion_data['yBoundaryList'])\n",
    "\n",
    "def getGRCIndex(x, y, xbst, ybst):\n",
    "    xi, yi = None, None\n",
    "    while isinstance(xbst, tuple):\n",
    "        if x is not None and x < xbst[0]:\n",
    "            xbst = xbst[1]\n",
    "        else:\n",
    "            xbst = xbst[2]\n",
    "        xi = xbst if not isinstance(xbst, tuple) else xi\n",
    "    \n",
    "    while isinstance(ybst, tuple):\n",
    "        if y is not None and y < ybst[0]:\n",
    "            ybst = ybst[1]\n",
    "        else:\n",
    "            ybst = ybst[2]\n",
    "        yi = ybst if not isinstance(ybst, tuple) else yi\n",
    "    \n",
    "    return xi, yi\n",
    "\n",
    "demand = np.zeros(len(instances))\n",
    "capacity = np.zeros(len(instances))\n",
    "demand_variance = np.zeros(len(instances))\n",
    "neighbor_demand = np.zeros(len(instances))\n",
    "\n",
    "layer_indices = {l: idx for idx, l in enumerate(congestion_data['layerList'])}\n",
    "iloc_jloc = instances.apply(lambda row: getGRCIndex(row['xloc'], row['yloc'], xbst, ybst), axis=1)\n",
    "iloc, jloc = zip(*iloc_jloc)\n",
    "for l in congestion_data['layerList']:\n",
    "    lyr = layer_indices[l]\n",
    "    layer_demand = congestion_data['demand'][lyr]\n",
    "    layer_capacity = congestion_data['capacity'][lyr]  # Assuming similar structure for capacity\n",
    "    layer_shape = layer_demand.shape\n",
    "\n",
    "    for k in range(len(instances)):\n",
    "        i, j = iloc[k], jloc[k]\n",
    "\n",
    "        demand_val = 0\n",
    "        capacity_val = 0  # Initialize capacity value for each instance\n",
    "        neighbor_vals = np.array([])\n",
    "\n",
    "        if 0 <= i < layer_shape[0] and 0 <= j < layer_shape[1]:\n",
    "            demand_val = layer_demand[i, j]\n",
    "            capacity_val = layer_capacity[i, j]  # Calculate capacity value\n",
    "\n",
    "            i_min, i_max = max(0, i-1), min(i+2, layer_shape[0])\n",
    "            j_min, j_max = max(0, j-1), min(j+2, layer_shape[1])\n",
    "            neighbor_vals = layer_demand[i_min:i_max, j_min:j_max].flatten()\n",
    "\n",
    "        demand[k] += demand_val\n",
    "        capacity[k] += capacity_val  # Accumulate capacity\n",
    "        demand_variance[k] = np.var(neighbor_vals) if neighbor_vals.size else 0\n",
    "        neighbor_demand[k] = np.mean(neighbor_vals) if neighbor_vals.size else 0\n",
    "\n",
    "\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    inertia = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    elbow_point = np.argmax(np.diff(inertia)) + 1\n",
    "    return elbow_point\n",
    "max_k = 10\n",
    "optimal_k = find_optimal_clusters(instances[['xloc', 'yloc']], max_k)\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42).fit(instances[['xloc', 'yloc']])\n",
    "centroids = kmeans.cluster_centers_\n",
    "x_grid_size = (centroids[:, 0].max() - centroids[:, 0].min()) / optimal_k\n",
    "y_grid_size = (centroids[:, 1].max() - centroids[:, 1].min()) / optimal_k\n",
    "instances['x_grid'] = instances['xloc'] // x_grid_size\n",
    "instances['y_grid'] = instances['yloc'] // y_grid_size\n",
    "spatial_features = instances.groupby(['x_grid', 'y_grid']).size().reset_index(name='grid_density')\n",
    "instances = instances.merge(spatial_features, how='left', on=['x_grid', 'y_grid'])\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "instances_encoded = pd.DataFrame(encoder.fit_transform(instances[['cell', 'orient']]).toarray())\n",
    "instances = instances.join(instances_encoded)\n",
    "\n",
    "G = nx.Graph(A)\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "clustering_coeff = nx.clustering(G)\n",
    "pagerank = nx.pagerank(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality_numpy(G)\n",
    "    \n",
    "degree = np.array(A.sum(axis=1)).flatten()\n",
    "\n",
    "instances['betweenness'] = instances.index.map(betweenness)\n",
    "instances['clustering_coeff'] = instances.index.map(clustering_coeff)\n",
    "instances['pagerank'] = instances.index.map(pagerank)\n",
    "instances['eigenvector_centrality'] = instances.index.map(eigenvector_centrality)\n",
    "instances['degree'] = degree\n",
    "instances['demand'] = demand\n",
    "instances['capacity'] = capacity\n",
    "instances['demand_variance'] = demand_variance\n",
    "instances['neighbor_demand'] = neighbor_demand\n",
    "instances['overflow'] = instances['demand'] - instances['capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdafd70-6c7b-457e-be16-904663560dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([                  'name',                     'id',\n",
      "                         'xloc',                   'yloc',\n",
      "                         'cell',                 'orient',\n",
      "                       'x_grid',                 'y_grid',\n",
      "                 'grid_density',                        0,\n",
      "                              1,                        2,\n",
      "                              3,                        4,\n",
      "                              5,                        6,\n",
      "                              7,                        8,\n",
      "                              9,                       10,\n",
      "                             11,                       12,\n",
      "                             13,                       14,\n",
      "                             15,                       16,\n",
      "                  'betweenness',       'clustering_coeff',\n",
      "                     'pagerank', 'eigenvector_centrality',\n",
      "                       'degree',                 'demand',\n",
      "                     'capacity',        'demand_variance',\n",
      "              'neighbor_demand',               'overflow'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(instances.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767751b5-9175-4f8a-91fa-a24f4a364f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/mvn599q51g5_76hcm5b9cjgw0000gn/T/ipykernel_81228/1830895938.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343668887/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  edge_index = torch.tensor([A_coo.row, A_coo.col], dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "one_hot_feature_columns = list(range(17))  # Assuming these are the indices of your one-hot encoded features\n",
    "original_feature_columns = [\n",
    "    'betweenness', 'clustering_coeff', 'pagerank', 'eigenvector_centrality',\n",
    "    'degree', 'demand', 'demand_variance', 'neighbor_demand'\n",
    "]\n",
    "feature_columns = original_feature_columns + one_hot_feature_columns\n",
    "label_column = 'overflow'\n",
    "features = torch.tensor(instances[feature_columns].values, dtype=torch.float).to(device)\n",
    "labels = torch.tensor(instances[label_column].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "A_coo = A.tocoo()\n",
    "edge_index = torch.tensor([A_coo.row, A_coo.col], dtype=torch.long).to(device)\n",
    "indices = np.arange(len(instances))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=23)\n",
    "train_mask = torch.zeros(len(instances), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(instances), dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "data = Data(x=features, edge_index=edge_index, y=labels, train_mask=train_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdb04c-1922-4a5f-a389-2ebffc29a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(num_features, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.layers.append(GCNConv(hidden_channels, 1))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7c4fb-8aee-4749-a523-8a96cbdde534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_features=len(feature_columns), hidden_channels=16, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4f3fb-250a-420e-9d64-96b4c27358b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_training\"):\n",
    "        model.train()\n",
    "        for epoch in range(15000):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)[data.train_mask]\n",
    "            loss = F.mse_loss(out, data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}: Training Loss = {loss.item()}')\n",
    "            if epoch == 14999:\n",
    "                train_pred_mean_congestion = out.mean().item()\n",
    "                train_true_mean_congestion = data.y[data.train_mask].mean().item()\n",
    "                print(f'Last Epoch {epoch}: Training True Mean Congestion = {train_true_mean_congestion}, Training Predicted Mean Congestion = {train_pred_mean_congestion}')\n",
    "                print(abs(train_true_mean_congestion - train_pred_mean_congestion))\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc99b5c-e968-4431-921f-ec0560356183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)[data.test_mask]\n",
    "    test_loss = F.mse_loss(out, data.y[data.test_mask]).item()\n",
    "    test_pred_mean_congestion = out.mean().item()\n",
    "    test_true_mean_congestion = data.y[data.test_mask].mean().item()\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test True Mean Congestion = {test_true_mean_congestion}, Test Predicted Mean Congestion = {test_pred_mean_congestion}')\n",
    "print(abs(test_true_mean_congestion - test_pred_mean_congestion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e56ac3-9e58-41e8-8770-a832187c20b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b46e7948-b8df-411b-b4cc-67dc3ff6f14a",
   "metadata": {},
   "source": [
    "# All xbar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5a654-d221-4807-8950-3cce038d4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_xbar(xbar_path):\n",
    "    def buildBST(array,start=0,finish=-1):\n",
    "        if finish<0:\n",
    "            finish = len(array)\n",
    "        mid = (start + finish) // 2\n",
    "        if mid-start==1:\n",
    "            ltl=start\n",
    "        else:\n",
    "            ltl=buildBST(array,start,mid)\n",
    "        \n",
    "        if finish-mid==1:\n",
    "            gtl=mid\n",
    "        else:\n",
    "            gtl=buildBST(array,mid,finish)\n",
    "            \n",
    "        return((array[mid],ltl,gtl))\n",
    "\n",
    "    def getGRCIndex(x,y,xbst,ybst):\n",
    "        while (type(xbst)==tuple):\n",
    "            if x < xbst[0]:\n",
    "                xbst=xbst[1]\n",
    "            else:\n",
    "                xbst=xbst[2]\n",
    "                \n",
    "        while (type(ybst)==tuple):\n",
    "            if y < ybst[0]:\n",
    "                ybst=ybst[1]\n",
    "            else:\n",
    "                ybst=ybst[2]\n",
    "                \n",
    "        return ybst, xbst\n",
    "    \n",
    "    with gzip.open(f'{xbar_path}/xbar.json.gz', 'rb') as f:\n",
    "        design = json.loads(f.read().decode('utf-8'))\n",
    "    instances = pd.DataFrame(design['instances'])\n",
    "    conn = np.load(f'{xbar_path}/xbar_connectivity.npz')\n",
    "    A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "    A = A.__mul__(A.T).tocoo()\n",
    "\n",
    "    congestion_data = np.load(f'{xbar_path}/xbar_congestion.npz')\n",
    "    xbst=buildBST(congestion_data['xBoundaryList'])\n",
    "    ybst=buildBST(congestion_data['yBoundaryList'])\n",
    "    demand = np.zeros(shape = [instances.shape[0],])\n",
    "    \n",
    "    for k in range(instances.shape[0]):\n",
    "        xloc = instances.iloc[k]['xloc']; yloc = instances.iloc[k]['yloc']\n",
    "        i,j=getGRCIndex(xloc,yloc,xbst,ybst)\n",
    "        d = 0 \n",
    "        for l in list(congestion_data['layerList']): \n",
    "            lyr=list(congestion_data['layerList']).index(l)\n",
    "            d += congestion_data['demand'][lyr][i][j]\n",
    "        demand[k] = d\n",
    "        \n",
    "    instances['routing_demand'] = demand\n",
    "    \n",
    "    capacity = congestion_data['capacity']\n",
    "    supply = np.zeros(shape=[instances.shape[0],])\n",
    "    \n",
    "    for k in range(instances.shape[0]):\n",
    "        xloc = instances.iloc[k]['xloc']\n",
    "        yloc = instances.iloc[k]['yloc']\n",
    "        i, j = getGRCIndex(xloc, yloc, xbst, ybst)\n",
    "        s = 0\n",
    "        for l in list(congestion_data['layerList']):\n",
    "            lyr = list(congestion_data['layerList']).index(l)\n",
    "            s += capacity[lyr][i][j]\n",
    "        supply[k] = s\n",
    "    instances['routing_supply'] = supply\n",
    "    instances['overflow'] = instances['routing_demand'] - instances['routing_supply']\n",
    "    instances['features'] = instances[['routing_demand', 'routing_supply']].values.tolist()\n",
    "    node_features = torch.tensor(instances['features'].values.tolist(), dtype=torch.float)\n",
    "    node_labels = torch.tensor(instances['overflow'].values, dtype=torch.float).unsqueeze(1)\n",
    "    edge_index = torch.tensor([A.row, A.col], dtype=torch.long)\n",
    "    return node_features, edge_index, node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4175a2-7734-4b24-8ae4-12d25802c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(num_features, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.layers.append(GCNConv(hidden_channels, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fc6b7-009d-4962-819b-fe1fe12b2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "all_xbars = ['xbar/' + str(i) for i in range(1, 14)]\n",
    "for test_xbar_path in all_xbars:\n",
    "    print(f\"Testing on {test_xbar_path}\")\n",
    "    test_data, test_edge_index, test_labels = load_and_process_xbar(test_xbar_path)\n",
    "    test_data = Data(x=test_data, edge_index=test_edge_index, y=test_labels).to(device)\n",
    "    train_data = [load_and_process_xbar(xbar_path) for xbar_path in all_xbars if xbar_path != test_xbar_path]\n",
    "    train_features = torch.cat([data[0] for data in train_data], dim=0)\n",
    "    train_labels = torch.cat([data[2] for data in train_data], dim=0)\n",
    "    train_edge_index = torch.cat([data[1] for data in train_data], dim=1)\n",
    "    train_data = Data(x=train_features, edge_index=train_edge_index, y=train_labels).to(device)\n",
    "    model = GCN(num_features=2, hidden_channels=16, num_layers=3).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        loss = F.mse_loss(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_data)\n",
    "        test_loss = F.mse_loss(out, test_data.y)\n",
    "        print(f\"Test Loss on {test_xbar_path}: {test_loss.item()}\")\n",
    "        results.append(test_loss.item())\n",
    "average_loss = sum(results) / len(results)\n",
    "print(f'Average Loss in Leave-One-Out Experiment: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3835c7-08f3-4506-8530-d91c2c78d8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
