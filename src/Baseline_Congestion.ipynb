{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e502def-944c-4e88-8ee6-e6264123cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db0418b-6a9d-49cf-917b-3ece2d8c9914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Srujan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('xbar/1/xbar.json.gz','rb') as f:\n",
    "    design = json.loads(f.read().decode('utf-8'))\n",
    "    \n",
    "instances = pd.DataFrame(design['instances'])\n",
    "nets = pd.DataFrame(design['nets'])\n",
    "\n",
    "conn=np.load('xbar/1/xbar_connectivity.npz')\n",
    "A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "A = A.__mul__(A.T)\n",
    "\n",
    "def buildBST(array,start=0,finish=-1):\n",
    "    if finish<0:\n",
    "        finish = len(array)\n",
    "    mid = (start + finish) // 2\n",
    "    if mid-start==1:\n",
    "        ltl=start\n",
    "    else:\n",
    "        ltl=buildBST(array,start,mid)\n",
    "    \n",
    "    if finish-mid==1:\n",
    "        gtl=mid\n",
    "    else:\n",
    "        gtl=buildBST(array,mid,finish)\n",
    "        \n",
    "    return((array[mid],ltl,gtl))\n",
    "\n",
    "congestion_data = np.load('xbar/1/xbar_congestion.npz')\n",
    "xbst=buildBST(congestion_data['xBoundaryList'])\n",
    "ybst=buildBST(congestion_data['yBoundaryList'])\n",
    "\n",
    "def getGRCIndex(x, y, xbst, ybst):\n",
    "    xi, yi = None, None\n",
    "    while isinstance(xbst, tuple):\n",
    "        if x is not None and x < xbst[0]:\n",
    "            xbst = xbst[1]\n",
    "        else:\n",
    "            xbst = xbst[2]\n",
    "        xi = xbst if not isinstance(xbst, tuple) else xi\n",
    "    \n",
    "    while isinstance(ybst, tuple):\n",
    "        if y is not None and y < ybst[0]:\n",
    "            ybst = ybst[1]\n",
    "        else:\n",
    "            ybst = ybst[2]\n",
    "        yi = ybst if not isinstance(ybst, tuple) else yi\n",
    "    \n",
    "    return xi, yi\n",
    "\n",
    "demand = np.zeros(len(instances))\n",
    "capacity = np.zeros(len(instances))\n",
    "demand_variance = np.zeros(len(instances))\n",
    "neighbor_demand = np.zeros(len(instances))\n",
    "\n",
    "layer_indices = {l: idx for idx, l in enumerate(congestion_data['layerList'])}\n",
    "iloc_jloc = instances.apply(lambda row: getGRCIndex(row['xloc'], row['yloc'], xbst, ybst), axis=1)\n",
    "iloc, jloc = zip(*iloc_jloc)\n",
    "for l in congestion_data['layerList']:\n",
    "    lyr = layer_indices[l]\n",
    "    layer_demand = congestion_data['demand'][lyr]\n",
    "    layer_capacity = congestion_data['capacity'][lyr]  # Assuming similar structure for capacity\n",
    "    layer_shape = layer_demand.shape\n",
    "\n",
    "    for k in range(len(instances)):\n",
    "        i, j = iloc[k], jloc[k]\n",
    "\n",
    "        demand_val = 0\n",
    "        capacity_val = 0  # Initialize capacity value for each instance\n",
    "        neighbor_vals = np.array([])\n",
    "\n",
    "        if 0 <= i < layer_shape[0] and 0 <= j < layer_shape[1]:\n",
    "            demand_val = layer_demand[i, j]\n",
    "            capacity_val = layer_capacity[i, j]  # Calculate capacity value\n",
    "\n",
    "            i_min, i_max = max(0, i-1), min(i+2, layer_shape[0])\n",
    "            j_min, j_max = max(0, j-1), min(j+2, layer_shape[1])\n",
    "            neighbor_vals = layer_demand[i_min:i_max, j_min:j_max].flatten()\n",
    "\n",
    "        demand[k] += demand_val\n",
    "        capacity[k] += capacity_val  # Accumulate capacity\n",
    "        demand_variance[k] = np.var(neighbor_vals) if neighbor_vals.size else 0\n",
    "        neighbor_demand[k] = np.mean(neighbor_vals) if neighbor_vals.size else 0\n",
    "\n",
    "\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    inertia = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    elbow_point = np.argmax(np.diff(inertia)) + 1\n",
    "    return elbow_point\n",
    "max_k = 10\n",
    "optimal_k = find_optimal_clusters(instances[['xloc', 'yloc']], max_k)\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42).fit(instances[['xloc', 'yloc']])\n",
    "centroids = kmeans.cluster_centers_\n",
    "x_grid_size = (centroids[:, 0].max() - centroids[:, 0].min()) / optimal_k\n",
    "y_grid_size = (centroids[:, 1].max() - centroids[:, 1].min()) / optimal_k\n",
    "instances['x_grid'] = instances['xloc'] // x_grid_size\n",
    "instances['y_grid'] = instances['yloc'] // y_grid_size\n",
    "spatial_features = instances.groupby(['x_grid', 'y_grid']).size().reset_index(name='grid_density')\n",
    "instances = instances.merge(spatial_features, how='left', on=['x_grid', 'y_grid'])\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "instances_encoded = pd.DataFrame(encoder.fit_transform(instances[['cell', 'orient']]).toarray())\n",
    "instances = instances.join(instances_encoded)\n",
    "\n",
    "G = nx.Graph(A)\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "clustering_coeff = nx.clustering(G)\n",
    "pagerank = nx.pagerank(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality_numpy(G)\n",
    "    \n",
    "degree = np.array(A.sum(axis=1)).flatten()\n",
    "\n",
    "instances['betweenness'] = instances.index.map(betweenness)\n",
    "instances['clustering_coeff'] = instances.index.map(clustering_coeff)\n",
    "instances['pagerank'] = instances.index.map(pagerank)\n",
    "instances['eigenvector_centrality'] = instances.index.map(eigenvector_centrality)\n",
    "instances['degree'] = degree\n",
    "instances['demand'] = demand\n",
    "instances['capacity'] = capacity\n",
    "instances['demand_variance'] = demand_variance\n",
    "instances['neighbor_demand'] = neighbor_demand\n",
    "instances['overflow'] = instances['demand'] - instances['capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdafd70-6c7b-457e-be16-904663560dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([                  'name',                     'id',\n",
      "                         'xloc',                   'yloc',\n",
      "                         'cell',                 'orient',\n",
      "                       'x_grid',                 'y_grid',\n",
      "                 'grid_density',                        0,\n",
      "                              1,                        2,\n",
      "                              3,                        4,\n",
      "                              5,                        6,\n",
      "                              7,                        8,\n",
      "                              9,                       10,\n",
      "                             11,                       12,\n",
      "                             13,                       14,\n",
      "                             15,                       16,\n",
      "                  'betweenness',       'clustering_coeff',\n",
      "                     'pagerank', 'eigenvector_centrality',\n",
      "                       'degree',                 'demand',\n",
      "                     'capacity',        'demand_variance',\n",
      "              'neighbor_demand',               'overflow'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(instances.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767751b5-9175-4f8a-91fa-a24f4a364f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srujan\\AppData\\Local\\Temp\\ipykernel_23460\\1830895938.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  edge_index = torch.tensor([A_coo.row, A_coo.col], dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "one_hot_feature_columns = list(range(17))  # Assuming these are the indices of your one-hot encoded features\n",
    "original_feature_columns = [\n",
    "    'betweenness', 'clustering_coeff', 'pagerank', 'eigenvector_centrality',\n",
    "    'degree', 'demand', 'demand_variance', 'neighbor_demand'\n",
    "]\n",
    "feature_columns = original_feature_columns + one_hot_feature_columns\n",
    "label_column = 'overflow'\n",
    "features = torch.tensor(instances[feature_columns].values, dtype=torch.float).to(device)\n",
    "labels = torch.tensor(instances[label_column].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "A_coo = A.tocoo()\n",
    "edge_index = torch.tensor([A_coo.row, A_coo.col], dtype=torch.long).to(device)\n",
    "indices = np.arange(len(instances))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=23)\n",
    "train_mask = torch.zeros(len(instances), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(instances), dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "data = Data(x=features, edge_index=edge_index, y=labels, train_mask=train_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cdb04c-1922-4a5f-a389-2ebffc29a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(num_features, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.layers.append(GCNConv(hidden_channels, 1))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94e7c4fb-8aee-4749-a523-8a96cbdde534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_features=len(feature_columns), hidden_channels=16, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4f3fb-250a-420e-9d64-96b4c27358b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 49167.69140625\n",
      "Epoch 100: Training Loss = 53.167449951171875\n",
      "Epoch 200: Training Loss = 39.67042541503906\n",
      "Epoch 300: Training Loss = 28.325939178466797\n",
      "Epoch 400: Training Loss = 24.533864974975586\n",
      "Epoch 500: Training Loss = 26.34563636779785\n",
      "Epoch 600: Training Loss = 23.44439697265625\n",
      "Epoch 700: Training Loss = 22.596332550048828\n",
      "Epoch 800: Training Loss = 21.59280014038086\n",
      "Epoch 900: Training Loss = 20.56842613220215\n",
      "Epoch 1000: Training Loss = 19.0261287689209\n",
      "Epoch 1100: Training Loss = 20.113557815551758\n",
      "Epoch 1200: Training Loss = 19.860567092895508\n",
      "Epoch 1300: Training Loss = 20.042978286743164\n",
      "Epoch 1400: Training Loss = 20.45949363708496\n",
      "Epoch 1500: Training Loss = 20.60371208190918\n",
      "Epoch 1600: Training Loss = 20.324556350708008\n",
      "Epoch 1700: Training Loss = 20.287059783935547\n",
      "Epoch 1800: Training Loss = 20.28780746459961\n",
      "Epoch 1900: Training Loss = 19.396387100219727\n",
      "Epoch 2000: Training Loss = 19.9892520904541\n",
      "Epoch 2100: Training Loss = 19.803192138671875\n",
      "Epoch 2200: Training Loss = 19.544044494628906\n",
      "Epoch 2300: Training Loss = 19.045095443725586\n",
      "Epoch 2400: Training Loss = 19.129980087280273\n",
      "Epoch 2500: Training Loss = 18.397069931030273\n",
      "Epoch 2600: Training Loss = 18.69985008239746\n",
      "Epoch 2700: Training Loss = 18.419689178466797\n",
      "Epoch 2800: Training Loss = 18.022733688354492\n",
      "Epoch 2900: Training Loss = 18.49742317199707\n",
      "Epoch 3000: Training Loss = 17.54744529724121\n",
      "Epoch 3100: Training Loss = 17.54551124572754\n",
      "Epoch 3200: Training Loss = 16.896081924438477\n",
      "Epoch 3300: Training Loss = 17.63705062866211\n",
      "Epoch 3400: Training Loss = 17.697162628173828\n",
      "Epoch 3500: Training Loss = 17.41266441345215\n",
      "Epoch 3600: Training Loss = 17.488454818725586\n",
      "Epoch 3700: Training Loss = 17.206754684448242\n",
      "Epoch 3800: Training Loss = 16.882665634155273\n",
      "Epoch 3900: Training Loss = 16.917640686035156\n",
      "Epoch 4000: Training Loss = 16.573741912841797\n",
      "Epoch 4100: Training Loss = 17.007144927978516\n",
      "Epoch 4200: Training Loss = 16.713613510131836\n",
      "Epoch 4300: Training Loss = 16.65127182006836\n",
      "Epoch 4400: Training Loss = 16.885910034179688\n",
      "Epoch 4500: Training Loss = 16.950632095336914\n",
      "Epoch 4600: Training Loss = 16.925668716430664\n",
      "Epoch 4700: Training Loss = 16.343576431274414\n",
      "Epoch 4800: Training Loss = 15.990229606628418\n",
      "Epoch 4900: Training Loss = 16.305768966674805\n",
      "Epoch 5000: Training Loss = 16.730022430419922\n",
      "Epoch 5100: Training Loss = 16.346357345581055\n",
      "Epoch 5200: Training Loss = 16.031370162963867\n",
      "Epoch 5300: Training Loss = 16.63296127319336\n",
      "Epoch 5400: Training Loss = 16.385997772216797\n",
      "Epoch 5500: Training Loss = 15.860452651977539\n",
      "Epoch 5600: Training Loss = 15.890837669372559\n",
      "Epoch 5700: Training Loss = 16.385534286499023\n",
      "Epoch 5800: Training Loss = 16.138996124267578\n",
      "Epoch 5900: Training Loss = 15.732646942138672\n",
      "Epoch 6000: Training Loss = 16.168201446533203\n",
      "Epoch 6100: Training Loss = 15.808683395385742\n",
      "Epoch 6200: Training Loss = 15.75033950805664\n",
      "Epoch 6300: Training Loss = 16.14464569091797\n",
      "Epoch 6400: Training Loss = 15.828702926635742\n",
      "Epoch 6500: Training Loss = 15.792475700378418\n",
      "Epoch 6600: Training Loss = 15.600359916687012\n",
      "Epoch 6700: Training Loss = 15.590628623962402\n",
      "Epoch 6800: Training Loss = 15.955819129943848\n",
      "Epoch 6900: Training Loss = 15.75073528289795\n",
      "Epoch 7000: Training Loss = 15.926804542541504\n",
      "Epoch 7100: Training Loss = 15.658056259155273\n",
      "Epoch 7200: Training Loss = 15.840049743652344\n",
      "Epoch 7300: Training Loss = 16.014970779418945\n",
      "Epoch 7400: Training Loss = 16.032073974609375\n",
      "Epoch 7500: Training Loss = 15.76318645477295\n",
      "Epoch 7600: Training Loss = 15.59984302520752\n",
      "Epoch 7700: Training Loss = 15.708832740783691\n",
      "Epoch 7800: Training Loss = 15.895410537719727\n",
      "Epoch 7900: Training Loss = 15.986067771911621\n",
      "Epoch 8000: Training Loss = 15.940006256103516\n",
      "Epoch 8100: Training Loss = 15.516945838928223\n",
      "Epoch 8200: Training Loss = 15.90109634399414\n",
      "Epoch 8300: Training Loss = 15.566719055175781\n",
      "Epoch 8400: Training Loss = 15.769073486328125\n",
      "Epoch 8500: Training Loss = 15.50250244140625\n",
      "Epoch 8600: Training Loss = 15.584240913391113\n",
      "Epoch 8700: Training Loss = 15.496931076049805\n",
      "Epoch 8800: Training Loss = 15.776986122131348\n",
      "Epoch 8900: Training Loss = 15.761343955993652\n",
      "Epoch 9000: Training Loss = 15.635424613952637\n",
      "Epoch 9100: Training Loss = 15.780426979064941\n",
      "Epoch 9200: Training Loss = 15.730156898498535\n",
      "Epoch 9300: Training Loss = 15.69972038269043\n",
      "Epoch 9400: Training Loss = 15.558923721313477\n",
      "Epoch 9500: Training Loss = 15.738597869873047\n",
      "Epoch 9600: Training Loss = 15.594162940979004\n",
      "Epoch 9700: Training Loss = 15.571864128112793\n",
      "Epoch 9800: Training Loss = 15.69393539428711\n",
      "Epoch 9900: Training Loss = 15.699419975280762\n",
      "Epoch 10000: Training Loss = 16.188453674316406\n",
      "Epoch 10100: Training Loss = 15.899051666259766\n",
      "Epoch 10200: Training Loss = 15.603955268859863\n",
      "Epoch 10300: Training Loss = 15.762734413146973\n",
      "Epoch 10400: Training Loss = 15.759854316711426\n",
      "Epoch 10500: Training Loss = 15.379122734069824\n",
      "Epoch 10600: Training Loss = 15.533843994140625\n",
      "Epoch 10700: Training Loss = 15.668780326843262\n",
      "Epoch 10800: Training Loss = 15.392938613891602\n",
      "Epoch 10900: Training Loss = 15.724879264831543\n",
      "Epoch 11000: Training Loss = 15.674551963806152\n",
      "Epoch 11100: Training Loss = 15.575270652770996\n",
      "Epoch 11200: Training Loss = 15.512041091918945\n",
      "Epoch 11300: Training Loss = 15.471844673156738\n",
      "Epoch 11400: Training Loss = 15.477039337158203\n",
      "Epoch 11500: Training Loss = 15.442361831665039\n",
      "Epoch 11600: Training Loss = 15.742742538452148\n",
      "Epoch 11700: Training Loss = 15.582744598388672\n",
      "Epoch 11800: Training Loss = 15.469259262084961\n",
      "Epoch 11900: Training Loss = 15.467935562133789\n",
      "Epoch 12000: Training Loss = 15.841146469116211\n",
      "Epoch 12100: Training Loss = 15.657258033752441\n",
      "Epoch 12200: Training Loss = 15.435224533081055\n",
      "Epoch 12300: Training Loss = 15.956644058227539\n",
      "Epoch 12400: Training Loss = 15.645322799682617\n",
      "Epoch 12500: Training Loss = 16.262649536132812\n",
      "Epoch 12600: Training Loss = 15.48033618927002\n",
      "Epoch 12700: Training Loss = 15.538434982299805\n",
      "Epoch 12800: Training Loss = 15.29293155670166\n",
      "Epoch 12900: Training Loss = 15.465116500854492\n",
      "Epoch 13000: Training Loss = 17.91828155517578\n",
      "Epoch 13100: Training Loss = 19.01581573486328\n",
      "Epoch 13200: Training Loss = 19.142072677612305\n",
      "Epoch 13300: Training Loss = 19.935909271240234\n",
      "Epoch 13400: Training Loss = 19.93486976623535\n",
      "Epoch 13500: Training Loss = 19.93012237548828\n",
      "Epoch 13600: Training Loss = 19.923978805541992\n",
      "Epoch 13700: Training Loss = 19.929607391357422\n",
      "Epoch 13800: Training Loss = 19.9295711517334\n",
      "Epoch 13900: Training Loss = 19.936655044555664\n",
      "Epoch 14000: Training Loss = 19.920608520507812\n",
      "Epoch 14100: Training Loss = 19.92644691467285\n",
      "Epoch 14200: Training Loss = 19.933237075805664\n",
      "Epoch 14300: Training Loss = 19.690622329711914\n",
      "Epoch 14400: Training Loss = 17.69545555114746\n",
      "Epoch 14500: Training Loss = 17.193891525268555\n",
      "Epoch 14600: Training Loss = 17.442337036132812\n",
      "Epoch 14700: Training Loss = 17.228792190551758\n",
      "Epoch 14800: Training Loss = 16.691017150878906\n",
      "Epoch 14900: Training Loss = 16.448598861694336\n",
      "Last Epoch 14999: Training True Mean Congestion = -7.609617233276367, Training Predicted Mean Congestion = -7.581830978393555\n",
      "0.0277862548828125\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "    with record_function(\"model_training\"):\n",
    "        model.train()\n",
    "        for epoch in range(15000):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)[data.train_mask]\n",
    "            loss = F.mse_loss(out, data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}: Training Loss = {loss.item()}')\n",
    "            if epoch == 14999:\n",
    "                train_pred_mean_congestion = out.mean().item()\n",
    "                train_true_mean_congestion = data.y[data.train_mask].mean().item()\n",
    "                print(f'Last Epoch {epoch}: Training True Mean Congestion = {train_true_mean_congestion}, Training Predicted Mean Congestion = {train_pred_mean_congestion}')\n",
    "                print(abs(train_true_mean_congestion - train_pred_mean_congestion))\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fc99b5c-e968-4431-921f-ec0560356183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 16.1758975982666\n",
      "Test True Mean Congestion = -7.857142925262451, Test Predicted Mean Congestion = -7.662431716918945\n",
      "0.19471120834350586\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)[data.test_mask]\n",
    "    test_loss = F.mse_loss(out, data.y[data.test_mask]).item()\n",
    "    test_pred_mean_congestion = out.mean().item()\n",
    "    test_true_mean_congestion = data.y[data.test_mask].mean().item()\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test True Mean Congestion = {test_true_mean_congestion}, Test Predicted Mean Congestion = {test_pred_mean_congestion}')\n",
    "print(abs(test_true_mean_congestion - test_pred_mean_congestion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e56ac3-9e58-41e8-8770-a832187c20b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b46e7948-b8df-411b-b4cc-67dc3ff6f14a",
   "metadata": {},
   "source": [
    "# All xbar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d5a654-d221-4807-8950-3cce038d4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_xbar(xbar_path):\n",
    "    def buildBST(array,start=0,finish=-1):\n",
    "        if finish<0:\n",
    "            finish = len(array)\n",
    "        mid = (start + finish) // 2\n",
    "        if mid-start==1:\n",
    "            ltl=start\n",
    "        else:\n",
    "            ltl=buildBST(array,start,mid)\n",
    "        \n",
    "        if finish-mid==1:\n",
    "            gtl=mid\n",
    "        else:\n",
    "            gtl=buildBST(array,mid,finish)\n",
    "            \n",
    "        return((array[mid],ltl,gtl))\n",
    "\n",
    "    def getGRCIndex(x,y,xbst,ybst):\n",
    "        while (type(xbst)==tuple):\n",
    "            if x < xbst[0]:\n",
    "                xbst=xbst[1]\n",
    "            else:\n",
    "                xbst=xbst[2]\n",
    "                \n",
    "        while (type(ybst)==tuple):\n",
    "            if y < ybst[0]:\n",
    "                ybst=ybst[1]\n",
    "            else:\n",
    "                ybst=ybst[2]\n",
    "                \n",
    "        return ybst, xbst\n",
    "    \n",
    "    with gzip.open(f'{xbar_path}/xbar.json.gz', 'rb') as f:\n",
    "        design = json.loads(f.read().decode('utf-8'))\n",
    "    instances = pd.DataFrame(design['instances'])\n",
    "    conn = np.load(f'{xbar_path}/xbar_connectivity.npz')\n",
    "    A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "    A = A.__mul__(A.T).tocoo()\n",
    "\n",
    "    congestion_data = np.load(f'{xbar_path}/xbar_congestion.npz')\n",
    "    xbst=buildBST(congestion_data['xBoundaryList'])\n",
    "    ybst=buildBST(congestion_data['yBoundaryList'])\n",
    "    demand = np.zeros(shape = [instances.shape[0],])\n",
    "    \n",
    "    for k in range(instances.shape[0]):\n",
    "        xloc = instances.iloc[k]['xloc']; yloc = instances.iloc[k]['yloc']\n",
    "        i,j=getGRCIndex(xloc,yloc,xbst,ybst)\n",
    "        d = 0 \n",
    "        for l in list(congestion_data['layerList']): \n",
    "            lyr=list(congestion_data['layerList']).index(l)\n",
    "            d += congestion_data['demand'][lyr][i][j]\n",
    "        demand[k] = d\n",
    "        \n",
    "    instances['routing_demand'] = demand\n",
    "    \n",
    "    capacity = congestion_data['capacity']\n",
    "    supply = np.zeros(shape=[instances.shape[0],])\n",
    "    \n",
    "    for k in range(instances.shape[0]):\n",
    "        xloc = instances.iloc[k]['xloc']\n",
    "        yloc = instances.iloc[k]['yloc']\n",
    "        i, j = getGRCIndex(xloc, yloc, xbst, ybst)\n",
    "        s = 0\n",
    "        for l in list(congestion_data['layerList']):\n",
    "            lyr = list(congestion_data['layerList']).index(l)\n",
    "            s += capacity[lyr][i][j]\n",
    "        supply[k] = s\n",
    "    instances['routing_supply'] = supply\n",
    "    instances['overflow'] = instances['routing_demand'] - instances['routing_supply']\n",
    "    instances['features'] = instances[['routing_demand', 'routing_supply']].values.tolist()\n",
    "    node_features = torch.tensor(instances['features'].values.tolist(), dtype=torch.float)\n",
    "    node_labels = torch.tensor(instances['overflow'].values, dtype=torch.float).unsqueeze(1)\n",
    "    edge_index = torch.tensor([A.row, A.col], dtype=torch.long)\n",
    "    return node_features, edge_index, node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4175a2-7734-4b24-8ae4-12d25802c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(GCNConv(num_features, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.layers.append(GCNConv(hidden_channels, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fc6b7-009d-4962-819b-fe1fe12b2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "all_xbars = ['xbar/' + str(i) for i in range(1, 14)]\n",
    "for test_xbar_path in all_xbars:\n",
    "    print(f\"Testing on {test_xbar_path}\")\n",
    "    test_data, test_edge_index, test_labels = load_and_process_xbar(test_xbar_path)\n",
    "    test_data = Data(x=test_data, edge_index=test_edge_index, y=test_labels).to(device)\n",
    "    train_data = [load_and_process_xbar(xbar_path) for xbar_path in all_xbars if xbar_path != test_xbar_path]\n",
    "    train_features = torch.cat([data[0] for data in train_data], dim=0)\n",
    "    train_labels = torch.cat([data[2] for data in train_data], dim=0)\n",
    "    train_edge_index = torch.cat([data[1] for data in train_data], dim=1)\n",
    "    train_data = Data(x=train_features, edge_index=train_edge_index, y=train_labels).to(device)\n",
    "    model = GCN(num_features=2, hidden_channels=16, num_layers=3).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        loss = F.mse_loss(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_data)\n",
    "        test_loss = F.mse_loss(out, test_data.y)\n",
    "        print(f\"Test Loss on {test_xbar_path}: {test_loss.item()}\")\n",
    "        results.append(test_loss.item())\n",
    "average_loss = sum(results) / len(results)\n",
    "print(f'Average Loss in Leave-One-Out Experiment: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3835c7-08f3-4506-8530-d91c2c78d8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
