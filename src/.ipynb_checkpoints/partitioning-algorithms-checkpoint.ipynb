{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix"
>>>>>>> origin
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> origin
   "metadata": {},
   "outputs": [],
   "source": [
    "def mincut_maxflow(G):\n",
    "    return ... #G.minimum_cut()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(G):\n",
    "    return nx.algorithms.community.kernighan_lin_bisection(G, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> origin
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "toy_graph = nx.Graph()\n"
=======
    "def calculate_initial_gain(G, node, partition):\n",
    "    internal, external = 0, 0\n",
    "    for neighbor in G[node]:\n",
    "        if neighbor in partition:\n",
    "            internal += G[node][neighbor].get('weight', 1)\n",
    "        else:\n",
    "            external += G[node][neighbor].get('weight', 1)\n",
    "    return external - internal\n",
    "\n",
    "def update_gains_and_queue(G, node, partition_a, partition_b, heap, node_to_heap):\n",
    "    for neighbor in G[node]:\n",
    "        if neighbor not in node_to_heap:\n",
    "            continue\n",
    "        if neighbor in partition_a:\n",
    "            new_gain = calculate_initial_gain(G, neighbor, partition_a)\n",
    "        else:\n",
    "            new_gain = calculate_initial_gain(G, neighbor, partition_b)\n",
    "        \n",
    "        heap_entry = node_to_heap[neighbor]\n",
    "        heap_entry[0] = -new_gain  # Negate for max-heap\n",
    "        heapq.heapify(heap)  # Re-heapify\n",
    "\n",
    "def is_balanced(partition_a, partition_b, max_imbalance):\n",
    "    size_a = len(partition_a)\n",
    "    size_b = len(partition_b)\n",
    "    return abs(size_a - size_b) <= max_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_partition(G, max_passes=10, max_imbalance=1):\n",
    "    nodes = list(G.nodes())  # Convert Nodes to a list\n",
    "    random.shuffle(nodes)  # Shuffle to ensure random partitioning\n",
    "    \n",
    "    partition_a, partition_b = set(nodes[:len(G)//2]), set(nodes[len(G)//2:])\n",
    "    heap = []\n",
    "    node_to_heap = {}\n",
    "\n",
    "    # Initialize gains and heap\n",
    "    for node in G:\n",
    "        gain = calculate_initial_gain(G, node, partition_a if node in partition_a else partition_b)\n",
    "        heap_entry = [-gain, node]  # Negate gain for max-heap\n",
    "        heapq.heappush(heap, heap_entry)\n",
    "        node_to_heap[node] = heap_entry\n",
    "\n",
    "    for _ in range(max_passes):\n",
    "        if not heap:\n",
    "            break\n",
    "\n",
    "        # Find a suitable node to move\n",
    "        while heap:\n",
    "            gain, node = heapq.heappop(heap)\n",
    "            gain = -gain  # Correct the negated gain\n",
    "\n",
    "            if node in partition_a and is_balanced(partition_a - {node}, partition_b | {node}, max_imbalance):\n",
    "                partition_a.remove(node)\n",
    "                partition_b.add(node)\n",
    "                break\n",
    "            elif node in partition_b and is_balanced(partition_a | {node}, partition_b - {node}, max_imbalance):\n",
    "                partition_b.remove(node)\n",
    "                partition_a.add(node)\n",
    "                break\n",
    "\n",
    "        if not heap:\n",
    "            # No suitable node found\n",
    "            break\n",
    "\n",
    "        # Lock this node\n",
    "        del node_to_heap[node]\n",
    "\n",
    "        # Update gains of neighbors\n",
    "        update_gains_and_queue(G, node, partition_a, partition_b, heap, node_to_heap)\n",
    "\n",
    "    return partition_a, partition_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subgraphs(G, partition_a, partition_b):\n",
    "    # Create subgraphs for each partition\n",
    "    subgraph_a = G.subgraph(partition_a).copy()\n",
    "    subgraph_b = G.subgraph(partition_b).copy()\n",
    "\n",
    "    return subgraph_a, subgraph_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_fm(G, subgraphs, k, max_imbalance):\n",
    "    \n",
    "    if k == 0:\n",
    "        subgraphs.append(G)\n",
    "        return G\n",
    "    \n",
    "    p_a, p_b = fm_partition(G, k, max_imbalance)\n",
    "    subgraph_a, subgraph_b = create_subgraphs(G, p_a, p_b)\n",
    "    \n",
    "    multiple_fm(subgraph_a, subgraphs, k - 1, max_imbalance)\n",
    "    multiple_fm(subgraph_b, subgraphs, k - 1, max_imbalance)\n",
    "    \n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = np.load('../data/xbar/1/xbar_connectivity.npz')\n",
    "A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "A = A.__mul__(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_tensors = from_scipy_sparse_matrix(A)\n",
    "edge_weights = edge_tensors[1]\n",
    "edge_list = [(u.item(),v.item(), weight) for u, v, weight in zip(edge_tensors[0][0], edge_tensors[0][1], edge_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_weighted_edges_from(edge_list)\n",
    "# G = nx.gnm_random_graph(1000, 2000)\n",
    "dag = nx.graph.deepcopy(G)\n",
    "subgraphs = []\n",
    "subgraph_partitions = 3\n",
    "max_imbalance = 10\n",
    "partition_output = multiple_fm(dag, [], subgraph_partitions, max_imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<networkx.classes.digraph.DiGraph at 0x162d60890>,\n",
       " <networkx.classes.digraph.DiGraph at 0x162c3c4d0>,\n",
       " <networkx.classes.digraph.DiGraph at 0x162aa4790>,\n",
       " <networkx.classes.digraph.DiGraph at 0x162997b90>,\n",
       " <networkx.classes.digraph.DiGraph at 0x178d5cbd0>,\n",
       " <networkx.classes.digraph.DiGraph at 0x178d5d890>,\n",
       " <networkx.classes.digraph.DiGraph at 0x1625c0450>,\n",
       " <networkx.classes.digraph.DiGraph at 0x1624c1ed0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 2967\n",
      "494 2542\n",
      "496 2566\n",
      "494 2720\n",
      "493 2575\n",
      "495 2525\n",
      "493 2515\n",
      "492 2734\n"
     ]
    }
   ],
   "source": [
    "for p in partition_output:\n",
    "    print(p.number_of_nodes(), p.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_dag(original_graph, reconstructed_dag, subgraph):\n",
    "    # Add nodes and edges from subgraph\n",
    "    for node in subgraph.nodes():\n",
    "        reconstructed_dag.add_node(node)\n",
    "    reconstructed_dag.add_edges_from(subgraph.edges())\n",
    "\n",
    "    # Add inter-partition edges from original graph\n",
    "    for u, v in original_graph.edges():\n",
    "        if u in subgraph or v in subgraph:\n",
    "            reconstructed_dag.add_edge(u, v, inter_partition=True)\n",
    "\n",
    "    return reconstructed_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resconstructed_dag = nx.DiGraph()\n",
    "# for i in range(len(partition_output)):\n",
    "#     reconstructed_dag = reconstruct_dag(G, resconstructed_dag, partition_output[i])"
>>>>>>> origin
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "def KL(G):\n",
    "    partitions = []\n",
    "    def split_graph(G):\n",
    "        return G.kernighan_lin_bisection(G, max_iter=100)\n",
    "    \n",
    "    return partitions"
   ]
=======
   "source": []
>>>>>>> origin
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
